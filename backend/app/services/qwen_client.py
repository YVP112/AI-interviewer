# app/services/qwen_client.py

import os
import re
from typing import Optional
from openai import OpenAI

from app.services.memory import memory  # ‚Üê –ü–†–ê–í–ò–õ–¨–ù–´–ô –ò–ú–ü–û–†–¢
from app.core.prompts import build_system_prompt
from app.services.tasks import get_task, random_task_by_level

# ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥


API_KEY = os.getenv("API_KEY")
BASE_URL = "https://llm.t1v.scibox.tech/"

client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
MODEL_NAME = "qwen3-coder-30b-a3b-instruct-fp8"

# –ú–∞–ø–ø–∏–Ω–≥ —É—Ä–æ–≤–Ω–µ–π –∏–Ω—Ç–µ—Ä–≤—å—é
LEVEL_NAMES = {
    1: "Junior",
    2: "Middle", 
    3: "Senior",
    4: "Expert"
}

def parse_coding_task(text: str) -> Optional[dict]:
    if not text:
        return None

    task_id_match = re.search(r"task_id:\s*([\w\-]+)", text, re.IGNORECASE)
    desc_match = re.search(
        r"description:\s*([\s\S]*?)(?:template:|```python)", text, re.IGNORECASE
    )
    template_match = (
        re.search(r"```python([\s\S]+?)```", text, re.IGNORECASE)
        or re.search(r"```([\s\S]+?)```", text, re.IGNORECASE)
    )

    if not task_id_match:
        return None

    return {
        "task_id": task_id_match.group(1).strip(),
        "description": desc_match.group(1).strip() if desc_match else "",
        "template": template_match.group(1).strip() if template_match else "",
    }

def make_final_report():
    system_prompt = (
        "–°—Ñ–æ—Ä–º–∏—Ä—É–π –∏—Ç–æ–≥–æ–≤–æ–µ —Ä–µ–∑—é–º–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤—å—é.\n\n"
        "–§–æ—Ä–º–∞—Ç —Å—Ç—Ä–æ–≥–æ —Ç–∞–∫–æ–π:\n"
        "**–¢–µ–æ—Ä–∏—è:** X%\n"
        "**–ü—Ä–∞–∫—Ç–∏–∫–∞:** Y%\n"
        "**–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:**\n"
        "‚Äî –ø—É–Ω–∫—Ç 1\n"
        "‚Äî –ø—É–Ω–∫—Ç 2\n"
        "**–ó–æ–Ω—ã —Ä–æ—Å—Ç–∞:**\n"
        "‚Äî –ø—É–Ω–∫—Ç 1\n"
        "‚Äî –ø—É–Ω–∫—Ç 2\n"
        "**–í–µ—Ä–¥–∏–∫—Ç:** <—Ç–µ–∫—Å—Ç>\n"
    )

    messages = [{"role": "system", "content": system_prompt}]
    messages.extend(memory.get_context())

    resp = client.chat.completions.create(
        model=MODEL_NAME,
        messages=messages,
        max_tokens=900,
        temperature=0.4,
    )

    return resp.choices[0].message.content

async def ask_qwen(message: str, mode: str, code_result: Optional[dict] = None):
    mode = (mode or "TECH").upper()
    memory.mode = mode

    # üîì –°–ï–ö–†–ï–¢–ù–ê–Ø –ö–û–ú–ê–ù–î–ê: –ø—Ä—è–º–æ–π –≤—Ö–æ–¥ –≤ –ø—Ä–∞–∫—Ç–∏–∫—É
    if message.lower().strip() == "–ø—Ä–∞–∫—Ç–∏–∫–∞ –æ—Ç –≤–∏—Ç—É—Å–∞":
        memory.reset_full()
        memory.stage = "practice-confirm"
        memory.interview_level = 2  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é Middle
        return {
            "answer": "üîì –†–µ–∂–∏–º –ø—Ä–∞–∫—Ç–∏–∫–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω! –ì–æ—Ç–æ–≤—ã –Ω–∞—á–∞—Ç—å solving tasks?",
            "next_task": None,
            "is_final": False,
            "secret_command": True
        }

    if mode == "TECH" and len(memory.history) == 0:
        memory.reset_full()

    # –≠–¢–ê–ü 1: INTRO - –≤—ã–±–æ—Ä —É—Ä–æ–≤–Ω—è –∏–Ω—Ç–µ—Ä–≤—å—é
    if memory.stage == "intro":
        memory.add_user_message(message)
        
        if message.lower() in ["–ø—Ä–∏–≤–µ—Ç", "–ø—Ä–∏–≤–µ—Ç!", "hi", "hello", "ok", "–æ–∫–µ–π", "–Ω–∞—á–∞—Ç—å", "–Ω–∞—á–Ω—ë–º", "–≥–æ—Ç–æ–≤"]:
            memory.stage = "level_select"
            response = (
                "–ü—Ä–∏–≤–µ—Ç! –ì–æ—Ç–æ–≤ –∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º—É –∏–Ω—Ç–µ—Ä–≤—å—é? üöÄ\n\n"
                "–í—ã–±–µ—Ä–∏ —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏:\n"
                "1Ô∏è‚É£ Junior ‚Äî –±–∞–∑–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –∑–∞–¥–∞—á–∏\n"
                "2Ô∏è‚É£ Middle ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã\n"
                "3Ô∏è‚É£ Senior ‚Äî —Å–ª–æ–∂–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã\n"
                "4Ô∏è‚É£ Expert ‚Äî —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å\n\n"
                "–û—Ç–≤–µ—Ç—å —Ü–∏—Ñ—Ä–æ–π (1, 2, 3 –∏–ª–∏ 4)"
            )
            memory.add_assistant_message(response)
            return {
                "answer": response,
                "next_task": None,
                "is_final": False
            }

        # –ò–Ω–∞—á–µ –æ—Ç–≤–µ—Ç–∏–º –∫–∞–∫ –æ–±—ã—á–Ω–æ –≤ intro —Ä–µ–∂–∏–º–µ
        system_prompt = build_system_prompt("TECH")
        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(memory.get_context())

        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            max_tokens=500,
            temperature=0.7,
        )
        answer = resp.choices[0].message.content
        memory.add_assistant_message(answer)
        return {"answer": answer, "next_task": None, "is_final": False}

    # –≠–¢–ê–ü 2: LEVEL_SELECT - –≤—ã–±—Ä–∞–Ω–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å
    if memory.stage == "level_select":
        memory.add_user_message(message)
        
        level_map = {"1": 1, "2": 2, "3": 3, "4": 4}
        level = level_map.get(message.strip())
        
        if level:
            memory.interview_level = level
            memory.coding_level = level  # –£—Ä–æ–≤–µ–Ω—å –∫–æ–¥–∏–Ω–≥–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —É—Ä–æ–≤–Ω—é –∏–Ω—Ç–µ—Ä–≤—å—é
            memory.stage = "theory"
            memory.theory_questions_asked = 0
            
            level_name = LEVEL_NAMES[level]
            response = (
                f"‚úÖ –£—Ä–æ–≤–µ–Ω—å **{level_name}** –≤—ã–±—Ä–∞–Ω!\n\n"
                "–ù–∞—á–∏–Ω–∞–µ–º —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫—É—é —á–∞—Å—Ç—å. –ù–∞ –∫–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å –æ—Ç–≤–µ—á–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ.\n\n"
                "–í–æ–ø—Ä–æ—Å 1Ô∏è‚É£: –†–∞—Å—Å–∫–∞–∂–∏ –æ —Ä–∞–∑–ª–∏—á–∏—è—Ö –º–µ–∂–¥—É mutable –∏ immutable —Ç–∏–ø–∞–º–∏ –≤ Python. "
                "–ü—Ä–∏–≤–µ–¥–∏ –ø—Ä–∏–º–µ—Ä—ã."
            )
            memory.add_assistant_message(response)
            memory.theory_questions_asked = 1
            return {"answer": response, "next_task": None, "is_final": False}
        
        # –ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥
        response = "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏ —É—Ä–æ–≤–µ–Ω—å —Ü–∏—Ñ—Ä–æ–π: 1, 2, 3 –∏–ª–∏ 4"
        memory.add_assistant_message(response)
        return {"answer": response, "next_task": None, "is_final": False}

    # FEEDBACK ‚Äî —Ä–∞–∑–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ coding-–∑–∞–¥–∞—á–∏
    if memory.stage == "feedback" and code_result is not None:
        hint_count = getattr(memory, "hint_count", 0)

        tests_text = "\n".join(code_result["results"])

        full_msg = (
            "–í–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:\n\n"
            f"{tests_text}\n\n"
            "–ï—Å–ª–∏ –≤—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã ‚Äî –ø–æ—Ö–≤–∞–ª–∏ –∏ –≤—ã–¥–∞–π —Å–ª–µ–¥—É—é—â—É—é –∑–∞–¥–∞—á—É —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ task_id/description/template.\n"
            "–ï—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∏ ‚Äî –¥–∞–π –û–î–ù–£ –º—è–≥–∫—É—é –ø–æ–¥—Å–∫–∞–∑–∫—É (–Ω–∞—á–∏–Ω–∞–π —Å–æ —Å–ª–æ–≤–∞ '–ú–æ–∂–µ—Ç...').\n"
            "–ü–æ—Å–ª–µ –¥–≤—É—Ö –Ω–µ—É–¥–∞—á–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ ‚Äî –∑–∞–≤–µ—Ä—à–∏ –∏–Ω—Ç–µ—Ä–≤—å—é –∏ –ø–æ–¥–≥–æ—Ç–æ–≤—å –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç.\n"
        )

        messages = [{"role": "system", "content": build_system_prompt("TECH")}]
        messages.extend(memory.get_context())
        messages.append({"role": "user", "content": full_msg})

        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            max_tokens=1200,
            temperature=0.4,
        )

        answer = resp.choices[0].message.content
        memory.add_assistant_message(answer)

        parsed = parse_coding_task(answer)

        if parsed:
            memory.stage = "coding"
            memory.current_task = parsed["task_id"]
            memory.hint_count = 0
            return {
                "answer": answer,
                "next_task": parsed,
                "is_final": False
            }
        else:
            memory.hint_count = hint_count + 1

        if memory.hint_count >= 2 and not code_result["success"]:
            final_report = make_final_report()
            memory.add_assistant_message(final_report)
            memory.reset_full()
            return {
                "answer": final_report,
                "next_task": None,
                "is_final": True
            }

        return {
            "answer": answer,
            "next_task": None,
            "is_final": False
        }

    # THEORY / CODING / PRACTICE_CONFIRM ‚Äî –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π –ø–æ—Ç–æ–∫
    if mode != "TECH":
        system_prompt = build_system_prompt(mode)
        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(memory.get_context())

        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            max_tokens=900,
            temperature=0.7,
        )
        answer = resp.choices[0].message.content
        memory.add_user_message(message)
        memory.add_assistant_message(answer)
        return {
            "answer": answer,
            "next_task": None,
            "is_final": False
        }

    # –¢–ï–û–†–ò–Ø: –∞–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–∞
    if memory.stage == "theory":
        memory.add_user_message(message)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞, –≥–æ—Ç–æ–≤ –ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç –ø–µ—Ä–µ–π—Ç–∏ –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ
        if message.lower() in ["–¥–∞", "–≥–æ—Ç–æ–≤", "–Ω–∞—á–∞—Ç—å", "–ª–∞–π–≤-–∫–æ–¥–∏–Ω–≥", "–ø—Ä–∞–∫—Ç–∏–∫–∞"]:
            if memory.theory_questions_asked >= 5:
                memory.stage = "practice_confirm"
                response = (
                    "–û—Ç–ª–∏—á–Ω–æ! –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\n\n"
                    "–ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —á–∞—Å—Ç–∏. –ë—É–¥–µ—Ç–µ —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏ –≤ live-coding.\n"
                    "–ì–æ—Ç–æ–≤—ã –Ω–∞—á–∞—Ç—å? –ù–∞–ø–∏—à–∏—Ç–µ: –¥–∞"
                )
                memory.add_assistant_message(response)
                return {"answer": response, "next_task": None, "is_final": False}
            else:
                response = (
                    f"–£ –Ω–∞—Å –µ—â—ë –µ—Å—Ç—å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã ({memory.theory_questions_asked}/5). "
                    "–î–∞–≤–∞–π—Ç–µ –ø—Ä–æ–¥–æ–ª–∂–∏–º!"
                )
                memory.add_assistant_message(response)
                return {"answer": response, "next_task": None, "is_final": False}

        # –ü–æ–ª—É—á–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å
        system_prompt = build_system_prompt("TECH")
        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(memory.get_context())

        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            max_tokens=800,
            temperature=0.6,
        )

        answer = resp.choices[0].message.content
        memory.add_assistant_message(answer)
        memory.theory_questions_asked += 1

        # –ü–æ—Å–ª–µ 5 –≤–æ–ø—Ä–æ—Å–æ–≤ –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ
        if memory.theory_questions_asked >= 5:
            answer = (
                answer + "\n\n"
                "---\n\n"
                "–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å –±–ª–∏–∑–∏—Ç—Å—è –∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é. "
                "–ì–æ—Ç–æ–≤—ã –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∫ live-coding? –ù–∞–ø–∏—à–∏—Ç–µ: –¥–∞"
            )

        return {"answer": answer, "next_task": None, "is_final": False}

    # PRACTICE_CONFIRM
    if memory.stage == "practice_confirm":
        memory.add_user_message(message)
        
        if message.lower() in ["–¥–∞", "–≥–æ—Ç–æ–≤", "–æ–∫", "–ø–æ–µ—Ö–∞–ª–∏", "–Ω–∞—á–∞—Ç—å"]:
            # –í—ã–±–∏—Ä–∞–µ–º –∑–∞–¥–∞—á—É –ø–æ —É—Ä–æ–≤–Ω—é –∫–æ–¥–∏–Ω–≥–∞
            coding_level = memory.coding_level
            task_id = random_task_by_level(coding_level)
            
            if task_id:
                task = get_task(task_id)
                memory.stage = "coding"
                memory.current_task = task_id
                memory.hint_count = 0
                
                response = (
                    f"üéØ –ó–∞–¥–∞—á–∞ —É—Ä–æ–≤–Ω—è **Level {coding_level}**:\n\n"
                    f"**{task['description']}**\n\n"
                    f"–í–∞—à —à–∞–±–ª–æ–Ω:\n"
                    f"```python\n{task['template']}\n```\n\n"
                    f"–ù–∞–ø–∏—à–∏—Ç–µ —Ä–µ—à–µ–Ω–∏–µ –≤ —Ä–µ–¥–∞–∫—Ç–æ—Ä–µ —Å–ª–µ–≤–∞."
                )
                
                memory.add_assistant_message(response)
                
                return {
                    "answer": response,
                    "next_task": {
                        "task_id": task_id,
                        "description": task['description'],
                        "template": task['template']
                    },
                    "is_final": False
                }

        # –ò–Ω–∞—á–µ –æ—Å—Ç–∞–≤–ª—è–µ–º—Å—è –≤ —Ä–µ–∂–∏–º–µ practice_confirm
        response = "–ö–æ–≥–¥–∞ –±—É–¥–µ—à—å –≥–æ—Ç–æ–≤, –Ω–∞–ø–∏—à–∏: –¥–∞"
        memory.add_assistant_message(response)
        return {"answer": response, "next_task": None, "is_final": False}

    # CODING: –∂–¥–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    if memory.stage == "coding":
        memory.add_user_message(message)
        response = "‚è≥ –ó–∞–ø—É—Å–∫–∞—é —Ç–µ—Å—Ç—ã –≤–∞—à–µ–≥–æ –∫–æ–¥–∞..."
        memory.add_assistant_message(response)
        return {"answer": response, "next_task": None, "is_final": False}

    # DEFAULT
    if mode != "TECH":
        system_prompt = build_system_prompt(mode)
        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(memory.get_context())

        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            max_tokens=900,
            temperature=0.7,
        )
        answer = resp.choices[0].message.content
        memory.add_assistant_message(answer)
        return {"answer": answer, "next_task": None, "is_final": False}

    # Fallback
    return {
        "answer": "–ò–∑–≤–∏–Ω–∏, —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑.",
        "next_task": None,
        "is_final": False
    }
